# ===========================
# core/brain.py â€” Bassam Ø§Ù„Ø°ÙƒÙŠ
# ===========================
import httpx, re
from bs4 import BeautifulSoup
from urllib.parse import quote

SEARCH_ENGINES = [
    ("Google", "https://www.google.com/search?q="),
    ("Bing", "https://www.bing.com/search?q="),
    ("DuckDuckGo", "https://duckduckgo.com/html?q="),
]

SOCIAL_PLATFORMS = {
    "Google": "https://www.google.com/search?q=",
    "Twitter/X": "https://twitter.com/search?q=",
    "Facebook": "https://www.facebook.com/search/people/?q=",
    "Instagram": "https://www.instagram.com/explore/search/keyword/?q=",
    "TikTok": "https://www.tiktok.com/search/user?q=",
    "LinkedIn": "https://www.linkedin.com/search/results/people/?keywords=",
    "Telegram": "https://t.me/s/",
    "Reddit": "https://www.reddit.com/search/?q=",
    "YouTube": "https://www.youtube.com/results?search_query=",
}


async def smart_search(q: str, want_social=False):
    """Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ø¹Ù…ÙŠÙ‚"""
    results = []
    q_encoded = quote(q)

    async with httpx.AsyncClient(follow_redirects=True, timeout=15) as client:
        # 1ï¸âƒ£ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ ÙÙŠ Ø¬ÙˆØ¬Ù„ ÙˆØ¨Ù†Ø¬ ÙˆØ¯Ùƒ Ø¯Ùƒ Ø¬Ùˆ
        for name, base in SEARCH_ENGINES:
            url = f"{base}{q_encoded}"
            try:
                r = await client.get(url, headers={"User-Agent": "Mozilla/5.0"})
                soup = BeautifulSoup(r.text, "html.parser")
                links = [a.get("href") for a in soup.find_all("a", href=True)]
                clean_links = [l for l in links if l and "http" in l and "google.com" not in l]
                results.extend(clean_links[:10])
            except:
                continue

    # 2ï¸âƒ£ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ø£ÙˆÙ„ Ø§Ù„ØµÙØ­Ø§Øª
    summaries = []
    async with httpx.AsyncClient(follow_redirects=True, timeout=10) as client:
        for link in results[:5]:
            try:
                r = await client.get(link, headers={"User-Agent": "Mozilla/5.0"})
                text = BeautifulSoup(r.text, "html.parser").get_text()
                text = re.sub(r"\s+", " ", text).strip()
                if len(text) > 300:
                    summaries.append(text[:400])
            except:
                pass

    # 3ï¸âƒ£ Ø¨Ø­Ø« Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
    social_links = []
    if want_social:
        for platform, base in SOCIAL_PLATFORMS.items():
            social_links.append(f"{platform}: {base}{q_encoded}")

    # ğŸ”¹ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    return {
        "answer": summaries[0] if summaries else "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ©.",
        "sources": results[:10],
        "social": social_links
    }
# âœ… Ø¯Ø§Ù„Ø© Ø°ÙƒÙŠØ© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© â€” ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„
async def smart_answer(query: str):
    """
    Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ù…Ø³Ø¤ÙˆÙ„Ø© Ø¹Ù† ØªÙ†ÙÙŠØ° Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„ÙƒØ§Ù…Ù„:
    1. ØªØ¨Ø­Ø« ÙÙŠ Ø§Ù„ÙˆÙŠØ¨ (GoogleØŒ BingØŒ Deep Web...)
    2. ØªØ¨Ø­Ø« ÙÙŠ ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ ÙˆØ§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.
    3. ØªØ¨Ø­Ø« ÙÙŠ Ø§Ù„Ø³ÙˆØ´Ø§Ù„ Ù…ÙŠØ¯ÙŠØ§ Ø¥Ù† ÙƒØ§Ù† Ø§Ù„Ø§Ø³Ù… Ø´Ø®ØµØ§Ù‹.
    4. ØªØ±Ø¬Ø¹ Ø¥Ø¬Ø§Ø¨Ø© Ø°ÙƒÙŠØ© + Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…ØµØ§Ø¯Ø±.
    """
    from core.search import deep_search  # ØªØ£ÙƒØ¯ Ø£Ù† Ù‡Ø°Ø§ Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù…Ø´Ø±ÙˆØ¹Ùƒ
    result = await deep_search(query)
    return result


# âœ… Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…Ø­Ù„ÙŠØ§Ù‹ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
def save_to_knowledge(question: str, answer: str):
    """
    ØªØ®Ø²Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ù…Ù„Ù knowledge.json Ø§Ù„Ù…Ø­Ù„ÙŠ.
    """
    import json, os
    path = "knowledge.json"
    data = []
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            try:
                data = json.load(f)
            except:
                data = []
    data.append({"question": question, "answer": answer})
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
