# core/brain.py â€” Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ù…Ø²Ø¯ÙˆØ¬: ÙÙ‡Ù… Ù…Ø­Ù„ÙŠ + Ø¨Ø­Ø« ÙˆÙŠØ¨ + ØªØ­Ù„ÙŠÙ„ ÙˆØªÙˆÙ„ÙŠØ¯ Ø°ÙƒÙŠ
from typing import List, Dict, Tuple
from pathlib import Path
import re
from rapidfuzz import fuzz, process
from duckduckgo_search import DDGS

# Ù…Ø³Ø§Ø± Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)
KB_FILE = DATA_DIR / "knowledge.txt"

if not KB_FILE.exists():
    KB_FILE.write_text("Ø³Ø¤Ø§Ù„: Ù…Ø§ ÙÙˆØ§Ø¦Ø¯ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©ØŸ\nØ¬ÙˆØ§Ø¨: Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© ØªÙˆØ³Ù‘Ø¹ Ø§Ù„Ù…Ø¯Ø§Ø±Ùƒ ÙˆØªÙ‚ÙˆÙ‘ÙŠ Ø§Ù„Ø®ÙŠØ§Ù„ ÙˆØªØ²ÙŠØ¯ Ø§Ù„Ø«Ù‚Ø§ÙØ©.\n---\n", encoding="utf-8")

# ======== Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ========
AR_DIAC = re.compile(r'[\u064B-\u0652]')
TOKEN_RE = re.compile(r'[A-Za-z\u0621-\u064A0-9]+')

def normalize_ar(s: str) -> str:
    s = s or ""
    s = AR_DIAC.sub("", s)
    s = s.replace("Ø£","Ø§").replace("Ø¥","Ø§").replace("Ø¢","Ø§")
    s = s.replace("Ø©","Ù‡").replace("Ù‰","ÙŠ").replace("Ø¤","Ùˆ").replace("Ø¦","ÙŠ")
    s = s.replace("Ú¯","Ùƒ").replace("Ù¾","Ø¨").replace("Ú¤","Ù")
    s = s.replace("Ø¸","Ø¶")
    s = re.sub(r"\s+"," ", s).strip()
    return s

def tokens(s: str) -> List[str]:
    return TOKEN_RE.findall(normalize_ar(s))

# ======== Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø­Ù„ÙŠØ© ========
def load_qa() -> List[Dict]:
    text = KB_FILE.read_text(encoding="utf-8")
    blocks = [b.strip() for b in text.split("---") if b.strip()]
    qa = []
    for b in blocks:
        m1 = re.search(r"Ø³Ø¤Ø§Ù„\s*:\s*(.+)", b)
        m2 = re.search(r"Ø¬ÙˆØ§Ø¨\s*:\s*(.+)", b)
        if m1 and m2:
            qa.append({"q": m1.group(1).strip(), "a": m2.group(1).strip()})
    return qa

QA = load_qa()
VOCAB = {w.lower() for qa in QA for w in tokens(qa["q"] + " " + qa["a"]) if len(w) > 2}

def correct_spelling_ar(text: str) -> str:
    toks = tokens(text)
    out = []
    for w in toks:
        lw = w.lower()
        if lw in VOCAB or len(lw) <= 2:
            out.append(lw); continue
        cand = process.extractOne(lw, VOCAB, scorer=fuzz.WRatio)
        out.append(cand[0] if cand and cand[1] >= 88 else lw)
    return " ".join(out)

def local_search(q: str) -> Tuple[Dict, float]:
    if not QA:
        return None, 0.0
    qn = correct_spelling_ar(q)
    best, score = None, 0.0
    for qa in QA:
        s = fuzz.token_set_ratio(qn, normalize_ar(qa["q"]))
        if s > score:
            best, score = qa, float(s)
    return best, score

# ======== Ø¨Ø­Ø« ÙˆÙŠØ¨ + ØªÙ„Ø®ÙŠØµ Ø°ÙƒÙŠ ========
def web_search(query: str, max_results: int = 6) -> List[Dict]:
    res = []
    with DDGS() as dd:
        for r in dd.text(query, max_results=max_results):
            res.append({"title": r.get("title"), "snippet": r.get("body"), "link": r.get("href")})
    return res

def _clean_lines(text: str, max_lines: int = 4) -> List[str]:
    if not text:
        return []
    lines = [l.strip(" .\t\r\n") for l in text.splitlines()]
    lines = [l for l in lines if 15 <= len(l) <= 220]
    seen, out = set(), []
    for l in lines:
        if l in seen: continue
        seen.add(l)
        out.append(l)
        if len(out) >= max_lines: break
    return out

def compose_web_answer(question: str, results: List[Dict]) -> Dict:
    bullets, links = [], []
    for r in results[:6]:
        title, snippet, link = r.get("title",""), r.get("snippet",""), r.get("link","")
        if link: links.append(link)
        if 15 <= len(title) <= 140:
            bullets.append(title.strip())
        bullets.extend(_clean_lines(snippet, max_lines=2))
    clean, seen = [], set()
    for b in bullets:
        if b and b not in seen:
            seen.add(b); clean.append(b)
        if len(clean) >= 10: break
    if not clean:
        return {"answer": "Ù„Ù… Ø£Ø¬Ø¯ Ù†Ù‚Ø§Ø·Ù‹Ø§ ÙˆØ§Ø¶Ø­Ø© Ù…Ù† Ø§Ù„ÙˆÙŠØ¨.", "links": links[:5]}
    head = f"Ø³Ø¤Ø§Ù„Ùƒ: {question}\n\nðŸ’¡ Ù…Ù„Ø®Øµ Ù…Ù† Ù…ØµØ§Ø¯Ø± Ø§Ù„ÙˆÙŠØ¨:\n"
    body = "\n".join([f"â€¢ {b}" for b in clean])
    tail = "\n\nðŸ”— Ø±ÙˆØ§Ø¨Ø·:\n" + "\n".join([f"- {u}" for u in links[:5]]) if links else ""
    return {"answer": head + body + tail, "links": links[:5]}

# ======== Ø§Ù„Ø¯Ù…Ø¬ Ø§Ù„Ø°ÙƒÙŠ ========
def smart_answer(question: str) -> Tuple[str, Dict]:
    q = (question or "").strip()
    if not q:
        return "Ù„Ù… Ø£Ø³ØªÙ„Ù… Ø³Ø¤Ø§Ù„Ù‹Ø§.", {"mode": "invalid"}

    doc, score = local_search(q)
    if doc and score >= 85:
        return doc["a"], {"mode": "local", "score": score, "match": doc["q"]}

    results = web_search(q, max_results=6)
    if results:
        pack = compose_web_answer(q, results)
        return pack["answer"], {"mode": "web", "links": pack.get("links", [])}

    if doc:
        return (
            f"Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¤ÙƒØ¯Ø©ØŒ Ù„ÙƒÙ† Ø£Ù‚Ø±Ø¨ Ø³Ø¤Ø§Ù„ ÙƒØ§Ù†: Â«{doc['q']}Â».\nØ§Ù„Ø¬ÙˆØ§Ø¨ Ø§Ù„Ù…Ø®Ø²Ù†: {doc['a']}"
        ), {"mode": "suggest", "score": score}

    return "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø¹Ø¯. Ø£Ø¶ÙÙ‡Ø§ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù„ØªØ­Ø³ÙŠÙ† Ø°ÙƒØ§Ø¡ Ø¨Ø³Ø§Ù….", {"mode": "none"}

def save_to_knowledge(q: str, a: str) -> None:
    if not q or not a:
        return
    with open(KB_FILE, "a", encoding="utf-8") as f:
        f.write(f"\nØ³Ø¤Ø§Ù„: {q}\nØ¬ÙˆØ§Ø¨: {a}\n---\n")
