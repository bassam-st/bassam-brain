# app.py â€” Bassam Brain (Ù…Ø­Ø±Ùƒ Ø£Ø³Ø¦Ù„Ø©/Ø£Ø¬ÙˆØ¨Ø© Ù…Ø­Ù„ÙŠ Ù…Ø¹ Ø£Ø¯ÙˆØ§Øª Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ø°ÙƒÙŠ)
from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import HTMLResponse, JSONResponse, FileResponse
import pathlib, re, json, time
from typing import List
from rapidfuzz import fuzz
from rank_bm25 import BM25Okapi
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

app = FastAPI(title="Bassam Brain â€“ Local QA")

# ================= Ù…Ù„ÙØ§Øª ÙˆØ°Ø§ÙƒØ±Ø© =================
DATA_DIR   = pathlib.Path("data");         DATA_DIR.mkdir(exist_ok=True)
NOTES_DIR  = DATA_DIR / "notes";           NOTES_DIR.mkdir(exist_ok=True)
LOG_FILE   = DATA_DIR / "log.jsonl"
FEED_FILE  = DATA_DIR / "feedback_pool.jsonl"
KB_FILE    = NOTES_DIR / "knowledge.txt"

if not KB_FILE.exists():
    KB_FILE.write_text("Ø³Ø¤Ø§Ù„: Ù…Ø§ ÙÙˆØ§Ø¦Ø¯ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©ØŸ\nØ¬ÙˆØ§Ø¨: Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© ØªÙˆØ³Ù‘Ø¹ Ø§Ù„Ù…Ø¯Ø§Ø±Ùƒ ÙˆØªÙ‚ÙˆÙ‘ÙŠ Ø§Ù„Ø®ÙŠØ§Ù„ ÙˆØªØ²ÙŠØ¯ Ø§Ù„Ø«Ù‚Ø§ÙØ©.\n---\n", encoding="utf-8")

# ================ Ø£Ø¯ÙˆØ§Øª Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ø°ÙƒÙŠ ================
AR_DIAC  = re.compile(r'[\u064B-\u0652]')
TOKEN_RE = re.compile(r'[A-Za-z\u0621-\u064A0-9]+')

def normalize_ar(s: str) -> str:
    """ØªØ·Ø¨ÙŠØ¹ Ø®ÙÙŠÙ Ù„Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„ØªÙ‚Ù„ÙŠÙ„ Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„ÙƒØªØ§Ø¨Ø©"""
    s = s or ""
    s = AR_DIAC.sub('', s)
    s = s.replace('Ø£','Ø§').replace('Ø¥','Ø§').replace('Ø¢','Ø§')
    s = s.replace('Ø©','Ù‡').replace('Ù‰','ÙŠ').replace('Ø¤','Ùˆ').replace('Ø¦','ÙŠ')
    s = s.replace('Ú¯','Ùƒ').replace('Ù¾','Ø¨').replace('Ú¤','Ù').replace('Ø¸','Ø¶')
    s = re.sub(r'\s+',' ', s).strip()
    return s

def ar_tokens(s: str) -> List[str]:
    return TOKEN_RE.findall(normalize_ar(s))

SYN_SETS = [
    {"Ù…Ù…ÙŠØ²Ø§Øª","ÙÙˆØ§Ø¦Ø¯","Ø§ÙŠØ¬Ø§Ø¨ÙŠØ§Øª","Ø­Ø³Ù†Ø§Øª"},
    {"Ø§Ø¶Ø±Ø§Ø±","Ø³Ù„Ø¨ÙŠØ§Øª","Ø¹ÙŠÙˆØ¨"},
    {"ØªØ¹Ø±ÙŠÙ","Ù…Ø§Ù‡Ùˆ","Ù…Ø§ Ù‡ÙŠ","Ù…ÙÙ‡ÙˆÙ…"},
    {"Ø§Ù†ØªØ±Ù†Øª","Ø§Ù„Ø´Ø¨ÙƒÙ‡","Ø§Ù„ÙˆÙŠØ¨","Ù†Øª"},
    {"Ù…ÙˆØ¨Ø§ÙŠÙ„","Ø¬ÙˆØ§Ù„","Ù‡Ø§ØªÙ"},
    {"Ø­Ø§Ø³ÙˆØ¨","ÙƒÙ…Ø¨ÙŠÙˆØªØ±","Ø­Ø§Ø³Ø¨Ù‡"},
    {"Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ","Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ","AI"},
    {"Ø¨Ø±Ù…Ø¬Ù‡","ØªÙƒÙˆÙŠØ¯","ÙƒÙˆØ¯","Ø¨Ø±Ù…Ø¬Ø©"},
    {"Ø§Ù…Ù† Ù…Ø¹Ù„ÙˆÙ…Ø§Øª","Ø§Ù…Ù† Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ","Ø­Ù…Ø§ÙŠÙ‡","Ø£Ù…Ù†"},
]

def expand_query(q: str) -> str:
    qn = normalize_ar(q)
    extra = []
    for syn in SYN_SETS:
        if any(w in qn for w in syn):
            extra.extend(list(syn))
    if extra:
        qn += " " + " ".join(extra)
    return qn

def load_qa() -> List[dict]:
    text = KB_FILE.read_text(encoding='utf-8') if KB_FILE.exists() else ""
    blocks = [b.strip() for b in text.split('---') if b.strip()]
    qa = []
    for b in blocks:
        m1 = re.search(r'Ø³Ø¤Ø§Ù„\s*:\s*(.+)', b)
        m2 = re.search(r'Ø¬ÙˆØ§Ø¨\s*:\s*(.+)', b)
        if m1 and m2:
            qa.append({"q": m1.group(1).strip(), "a": m2.group(1).strip()})
    return qa

QA_CACHE = load_qa()

def build_indexes(qa_list: List[dict]):
    if not qa_list:
        return None, [], None, None
    docs_tokens = [ar_tokens(x["q"] + " " + x["a"]) for x in qa_list]
    bm25 = BM25Okapi(docs_tokens)
    corpus_norm = [normalize_ar(x["q"]) for x in qa_list]
    tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3,5), min_df=1)
    X = tfidf.fit_transform(corpus_norm)
    return bm25, docs_tokens, tfidf, X

BM25, BM25_DOCS, TFVEC, X_TFIDF = build_indexes(QA_CACHE)

def reload_kb():
    global QA_CACHE, BM25, BM25_DOCS, TFVEC, X_TFIDF
    QA_CACHE = load_qa()
    BM25, BM25_DOCS, TFVEC, X_TFIDF = build_indexes(QA_CACHE)

def combined_search(user_q: str, topk: int = 5):
    if not QA_CACHE:
        return None, 0.0
    q_expanded = expand_query(user_q)
    q_norm = normalize_ar(q_expanded)
    q_tok  = ar_tokens(q_norm)
    scores = {}
    if BM25:
        bm = BM25.get_scores(q_tok)
        for i, s in enumerate(bm):
            if s > 0:
                scores[i] = scores.get(i, 0.0) + s * 10.0
    for i, qa in enumerate(QA_CACHE):
        s = fuzz.token_set_ratio(q_norm, normalize_ar(qa["q"]))
        if s > 0:
            scores[i] = scores.get(i, 0.0) + float(s)
    if TFVEC is not None:
        q_vec = TFVEC.transform([q_norm])
        cos = X_TFIDF @ q_vec.T
        cos = np.asarray(cos.todense()).ravel()
        for i, s in enumerate(cos):
            if s > 0:
                scores[i] = scores.get(i, 0.0) + float(s) * 120.0
    if not scores:
        return None, 0.0
    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:topk]
    best_idx, best_score = ranked[0]
    return QA_CACHE[best_idx], best_score

async def local_qa_answer(question: str) -> str:
    question = (question or "").strip()
    if not question:
        return "âš ï¸ Ù„Ù… Ø£Ø³ØªÙ„Ù… Ø³Ø¤Ø§Ù„Ù‹Ø§."
    doc, score = combined_search(question, topk=5)
    if doc and score >= 140:
        return doc["a"]
    if doc and score >= 90:
        return f"{doc['a']}\n\n(â„¹ï¸ Ø£Ù‚Ø±Ø¨ Ø³Ø¤Ø§Ù„ Ù…Ø´Ø§Ø¨Ù‡: Â«{doc['q']}Â»)"
    if doc:
        return f"â“ Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¤ÙƒØ¯Ø©ØŒ Ù„ÙƒÙ† Ø§Ù„Ø£Ù‚Ø±Ø¨ Ù‡Ùˆ:\nÂ«{doc['q']}Â».\nØ§Ù„Ø¬ÙˆØ§Ø¨: {doc['a']}\n\nÙŠÙ…ÙƒÙ†Ùƒ Ø­ÙØ¸ Ø³Ø¤Ø§Ù„Ùƒ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø°ÙƒØ§Ø¡ Ù„Ø§Ø­Ù‚Ù‹Ø§."
    return "ğŸ¤– Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ø¨Ø¹Ø¯ Ù„Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„."

# ================= Ø§Ù„ØµÙØ­Ø§Øª =================
@app.get("/", response_class=HTMLResponse)
def home():
    return """
    <div style="max-width:760px;margin:24px auto;font-family:system-ui">
      <h2>ğŸ§  Bassam Brain â€” Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ù…Ø­Ù„ÙŠ Ø§Ù„Ø°ÙƒÙŠ</h2>
      <form method="post" action="/ask">
        <textarea name="q" rows="4" style="width:100%" placeholder="Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§..."></textarea>
        <button style="margin-top:8px;padding:8px 16px;background:#007bff;color:white;border:none;border-radius:6px">Ø¥Ø±Ø³Ø§Ù„</button>
      </form>
    </div>
    """

@app.post("/ask", response_class=HTMLResponse)
async def ask(request: Request):
    form = await request.form()
    q = (form.get("q") or "").strip()
    ans = await local_qa_answer(q)
    rec = {"ts": int(time.time()), "q": q, "a": ans}
    with open(LOG_FILE, "a", encoding="utf-8") as f: f.write(json.dumps(rec, ensure_ascii=False) + "\n")
    return f"""
    <div style='max-width:720px;margin:24px auto;font-family:system-ui'>
      <p><b>Ø³Ø¤Ø§Ù„Ùƒ:</b> {q}</p>
      <p><b>Ø§Ù„Ø¬ÙˆØ§Ø¨:</b> {ans}</p>

      <form method='post' action='/save_to_knowledge'>
        <input type='hidden' name='q' value='{q}'>
        <input type='hidden' name='a' value='{ans}'>
        <button style='background:#28a745;color:white;padding:8px 16px;border:none;border-radius:6px'>âœ… Ø­ÙØ¸ Ø§Ù„Ø¬ÙˆØ§Ø¨ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©</button>
      </form>

      <p style='margin-top:16px'><a href='/'>â—€ Ø±Ø¬ÙˆØ¹</a></p>
    </div>
    """

@app.post("/save_to_knowledge", response_class=HTMLResponse)
async def save_to_knowledge(request: Request):
    form = await request.form()
    q = (form.get("q") or "").strip()
    a = (form.get("a") or "").strip()
    if not q or not a:
        return "<p>âš ï¸ ÙŠØ±Ø¬Ù‰ ÙƒØªØ§Ø¨Ø© Ø³Ø¤Ø§Ù„ ÙˆØ¬ÙˆØ§Ø¨ Ø£ÙˆÙ„Ø§Ù‹.</p><p><a href='/'>â—€ Ø±Ø¬ÙˆØ¹</a></p>"
    with open(KB_FILE, "a", encoding="utf-8") as f:
        f.write(f"\nØ³Ø¤Ø§Ù„: {q}\nØ¬ÙˆØ§Ø¨: {a}\n---\n")
    reload_kb()
    return "<p>âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¬ÙˆØ§Ø¨ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø¨Ù†Ø¬Ø§Ø­.</p><p><a href='/'>â—€ Ø±Ø¬ÙˆØ¹</a></p>"

@app.get("/ready")
def ready(): return {"ok": True}
