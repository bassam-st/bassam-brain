# app.py â€” Bassam Brain (Ù…Ø­Ù„ÙŠ + Ù‚Ø§Ù…ÙˆØ³ ØªØµØ­ÙŠØ­ + ÙÙ‡Ù… Ø°ÙƒÙŠ)
from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import HTMLResponse, JSONResponse, FileResponse
import pathlib, re, json, time
from typing import List, Dict, Set, Tuple

app = FastAPI(title="Bassam Brain â€“ Local QA (Smart)")

# ========= Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª =========
DATA_DIR   = pathlib.Path("data");         DATA_DIR.mkdir(exist_ok=True)
NOTES_DIR  = DATA_DIR / "notes";           NOTES_DIR.mkdir(exist_ok=True)
DICT_DIR   = DATA_DIR / "dict";            DICT_DIR.mkdir(exist_ok=True)

LOG_FILE   = DATA_DIR / "log.jsonl"
FEED_FILE  = DATA_DIR / "feedback_pool.jsonl"
KB_FILE    = NOTES_DIR / "knowledge.txt"
TYPOS_FILE = DICT_DIR / "typos.tsv"
SYN_FILE   = DICT_DIR / "synonyms.txt"

# Ù…Ù„Ù Ù…Ø¹Ø±ÙØ© Ø§ÙØªØ±Ø§Ø¶ÙŠ
if not KB_FILE.exists():
    KB_FILE.write_text("Ø³Ø¤Ø§Ù„: Ù…Ø§ ÙÙˆØ§Ø¦Ø¯ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©ØŸ\nØ¬ÙˆØ§Ø¨: Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© ØªÙˆØ³Ù‘Ø¹ Ø§Ù„Ù…Ø¯Ø§Ø±Ùƒ ÙˆØªÙ‚ÙˆÙ‘ÙŠ Ø§Ù„Ø®ÙŠØ§Ù„ ÙˆØªØ²ÙŠØ¯ Ø§Ù„Ø«Ù‚Ø§ÙØ©.\n---\n", encoding="utf-8")

# Ù…Ù„ÙØ§Øª Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
if not TYPOS_FILE.exists():
    TYPOS_FILE.write_text(
        "# wrong<TAB>right  (Ø³Ø·Ø± ÙŠØ¨Ø¯Ø£ Ø¨Ù€# ÙŠÙÙ‡Ù…Ù„)\n"
        "Ø²ÙƒØ§Ø¡\tØ°ÙƒØ§Ø¡\n"
        "Ø§Ù„Ø²ÙƒØ§Ø¡\tØ§Ù„Ø°ÙƒØ§Ø¡\n"
        "ØªØ¹ÙŠÙ\tØªØ¹Ø±ÙŠÙ\n"
        "Ø¨Ø±Ù…Ø¬Ù‡\tØ¨Ø±Ù…Ø¬Ø©\n"
        "Ø§Ù†ØªØ± Ù†Øª\tØ§Ù†ØªØ±Ù†Øª\n"
        "Ø§Ù„Ù†Øª\tØ§Ù†ØªØ±Ù†Øª\n"
        "ÙØ§ÙŠØ¯Ù‡\tÙØ§Ø¦Ø¯Ø©\n"
        "ÙÙˆÙŠØ¯\tÙÙˆØ§Ø¦Ø¯\n"
        "Ø§Ù„Ù‚Ø±Ø§Ø¡Ù‡\tØ§Ù„Ù‚Ø±Ø§Ø¡Ø©\n"
        "Ø§Ù„Ø²Ù…Ù†ÙŠÙ‡\tØ§Ù„Ø²Ù…Ù†ÙŠØ©\n"
        "Ø§Ù„Ø§Ø¶Ø·Ù†Ø§Ø¹ÙŠ\tØ§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ\n"
        , encoding="utf-8"
    )

if not SYN_FILE.exists():
    SYN_FILE.write_text(
        "# Ø§ÙƒØªØ¨ ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ø±Ø§Ø¯ÙØ§Øª ÙÙŠ Ø³Ø·Ø± Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„\n"
        "ÙÙˆØ§Ø¦Ø¯,Ù…Ù…ÙŠØ²Ø§Øª,Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ§Øª,Ø­Ø³Ù†Ø§Øª\n"
        "Ø£Ø¶Ø±Ø§Ø±,Ø³Ù„Ø¨ÙŠØ§Øª,Ø¹ÙŠÙˆØ¨\n"
        "ØªØ¹Ø±ÙŠÙ,Ù…Ø§Ù‡Ùˆ,Ù…Ø§ Ù‡ÙŠ,Ù…ÙÙ‡ÙˆÙ…\n"
        "Ø§Ù†ØªØ±Ù†Øª,Ø´Ø¨ÙƒØ©,Ø§Ù„ÙˆÙŠØ¨,Ù†Øª\n"
        "Ù…ÙˆØ¨Ø§ÙŠÙ„,Ø¬ÙˆØ§Ù„,Ù‡Ø§ØªÙ\n"
        "Ø­Ø§Ø³ÙˆØ¨,ÙƒÙ…Ø¨ÙŠÙˆØªØ±,Ø­Ø§Ø³Ø¨Ø©\n"
        "Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ,Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ,AI\n"
        "Ø¨Ø±Ù…Ø¬Ø©,ØªÙƒÙˆÙŠØ¯,ÙƒÙˆØ¯\n"
        "Ø£Ù…Ù† Ù…Ø¹Ù„ÙˆÙ…Ø§Øª,Ø£Ù…Ù† Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ,Ø­Ù…Ø§ÙŠØ©\n"
        , encoding="utf-8"
    )

# ========= Ø£Ø¯ÙˆØ§Øª NLP Ù…Ø­Ù„ÙŠØ© =========
from rapidfuzz import fuzz, process
from rank_bm25 import BM25Okapi
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# --- ØªØ·Ø¨ÙŠØ¹ Ø¹Ø±Ø¨ÙŠ Ø®ÙÙŠÙ
AR_DIAC  = re.compile(r'[\u064B-\u0652]')                # Ø§Ù„ØªØ´ÙƒÙŠÙ„
TOKEN_RE = re.compile(r'[A-Za-z\u0621-\u064A0-9]+')

def normalize_ar(s: str) -> str:
    s = s or ""
    s = AR_DIAC.sub('', s)
    s = s.replace('Ø£','Ø§').replace('Ø¥','Ø§').replace('Ø¢','Ø§')
    s = s.replace('Ø©','Ù‡').replace('Ù‰','ÙŠ').replace('Ø¤','Ùˆ').replace('Ø¦','ÙŠ')
    s = s.replace('Ú¯','Ùƒ').replace('Ù¾','Ø¨').replace('Ú¤','Ù')
    s = s.replace('Ø¸','Ø¶')  # Ù†ØªØ³Ø§Ù…Ø­ Ø§Ù„Ø¶/Ø¸
    s = re.sub(r'\s+',' ', s).strip()
    return s

def ar_tokens(s: str) -> List[str]:
    return TOKEN_RE.findall(normalize_ar(s))

# --- ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© (Ø³/Ø¬)
def load_qa() -> List[dict]:
    text = KB_FILE.read_text(encoding='utf-8') if KB_FILE.exists() else ""
    blocks = [b.strip() for b in text.split('---') if b.strip()]
    qa = []
    for b in blocks:
        m1 = re.search(r'Ø³Ø¤Ø§Ù„\s*:\s*(.+)', b)
        m2 = re.search(r'Ø¬ÙˆØ§Ø¨\s*:\s*(.+)', b)
        if m1 and m2:
            q = m1.group(1).strip()
            a = m2.group(1).strip()
            qa.append({"q": q, "a": a})
    return qa

QA_CACHE: List[dict] = load_qa()

# --- ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ø¨Ø­Ø« (BM25 + TF-IDF)
def build_indexes(qa_list: List[dict]):
    if not qa_list:
        return None, [], None, None
    docs_tokens = [ar_tokens(x["q"] + " " + x["a"]) for x in qa_list]
    bm25 = BM25Okapi(docs_tokens)

    corpus_norm = [normalize_ar(x["q"]) for x in qa_list]
    tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3,5), min_df=1)
    X = tfidf.fit_transform(corpus_norm)
    return bm25, docs_tokens, tfidf, X

BM25, BM25_DOCS, TFVEC, X_TFIDF = build_indexes(QA_CACHE)

# --- Ù‚Ø§Ù…ÙˆØ³: Ù…Ø±Ø§Ø¯ÙØ§Øª + ØªØµØ­ÙŠØ­Ø§Øª + Ø®Ø±ÙŠØ·Ø© Ù„Ø¨Ø³ Ø­Ø±ÙˆÙ
SYN_SETS: List[Set[str]] = []
TYPOS_MAP: Dict[str,str] = {}

# Ù„Ø¨Ø³ Ø§Ù„Ø­Ø±ÙˆÙ (Ù†Ø³ØªØ®Ø¯Ù…Ù‡ Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø±Ø´Ø­ÙŠÙ† Ø­Ø±ÙÙŠÙ‹Ø§ Ø«Ù… Ù†ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯Ù‡Ù… ÙÙŠ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª)
CONFUSION_MAP: Dict[str, List[str]] = {
    'Ø³':['Øµ'], 'Øµ':['Ø³'],
    'Ø²':['Ø°'], 'Ø°':['Ø²'],
    'Øª':['Ø«'], 'Ø«':['Øª'],
    'Ø¯':['Ø°'], 'Ø·':['Øª'],
    'Ù‚':['Øº'], 'Øº':['Ù‚'],
    'Ø­':['Ø®','Ø¬'], 'Ø¬':['Ø­','Ø®'], 'Ø®':['Ø­','Ø¬'],
    'Ø¶':['Ø¸'],  # Ø§Ù„Ø¸ Ø·ÙØ¨Ù‘Ø¹Øª Ø¥Ù„Ù‰ Ø¶ØŒ Ù†Ø³Ù…Ø­ Ø¨Ø§Ù„Ø¹ÙƒØ³ Ø¹Ù†Ø¯ Ø§Ù„ØªÙˆÙ„ÙŠØ¯
}

def load_synonyms():
    global SYN_SETS
    SYN_SETS = []
    if SYN_FILE.exists():
        for line in SYN_FILE.read_text(encoding='utf-8').splitlines():
            line=line.strip()
            if not line or line.startswith('#'): continue
            words = [normalize_ar(w) for w in line.split(',') if w.strip()]
            if len(words)>=2:
                SYN_SETS.append(set(words))
    # fallback Ø¨Ø³ÙŠØ· Ù„Ùˆ Ø§Ù„Ù…Ù„Ù ÙØ§Ø¶ÙŠ
    if not SYN_SETS:
        SYN_SETS = [
            {"ÙÙˆØ§Ø¦Ø¯","Ù…Ù…ÙŠØ²Ø§Øª","Ø§ÙŠØ¬Ø§Ø¨ÙŠØ§Øª","Ø­Ø³Ù†Ø§Øª"},
            {"Ø§Ø¶Ø±Ø§Ø±","Ø³Ù„Ø¨ÙŠØ§Øª","Ø¹ÙŠÙˆØ¨"},
            {"ØªØ¹Ø±ÙŠÙ","Ù…Ø§Ù‡Ùˆ","Ù…Ø§ Ù‡ÙŠ","Ù…ÙÙ‡ÙˆÙ…"},
        ]

def load_typos():
    global TYPOS_MAP
    TYPOS_MAP = {}
    if TYPOS_FILE.exists():
        for line in TYPOS_FILE.read_text(encoding='utf-8').splitlines():
            line=line.strip()
            if not line or line.startswith('#'): continue
            if '\t' not in line: continue
            wrong,right = line.split('\t',1)
            wrong = normalize_ar(wrong)
            right = normalize_ar(right)
            if wrong and right:
                TYPOS_MAP[wrong]=right

load_synonyms(); load_typos()

# --- Ù…ÙØ±Ø¯Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© (Ù„Ù„Ù…ØµØ­Ù‘Ø­)
VOCAB: Set[str] = set()
def build_vocab():
    global VOCAB
    VOCAB = set()
    for qa in QA_CACHE:
        for w in ar_tokens(qa["q"] + " " + qa["a"]):
            if len(w) > 2:
                VOCAB.add(w.lower())
    # Ù†Ø¶ÙŠÙ Ù…Ø±Ø§Ø¯ÙØ§Øª Ø£ÙŠØ¶Ø§Ù‹ Ù„Ù„ÙØ§Ø¦Ø¯Ø©
    for syn in SYN_SETS:
        for w in syn:
            if len(w)>2: VOCAB.add(w.lower())

build_vocab()

def expand_query(q: str) -> str:
    qn = normalize_ar(q)
    extra = []
    for syn in SYN_SETS:
        if any(w in qn for w in syn):
            extra.extend(list(syn))
    if extra:
        qn += " " + " ".join(extra)
    return qn

def generate_confusion_candidates(token: str) -> List[str]:
    """ØªÙˆÙ„ÙŠØ¯ Ù…Ø±Ø´Ø­ÙŠÙ† Ø¨ØªØ¨Ø¯ÙŠÙ„ Ø­Ø±Ù ÙˆØ§Ø­Ø¯ ÙˆÙÙ‚ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù„Ø¨Ø³ Ø«Ù… Ù†ØªØ­Ù‚Ù‚ Ù‡Ù„ Ø§Ù„Ù…Ø±Ø´Ø­ Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª."""
    cands=set()
    chars=list(token)
    for i,ch in enumerate(chars):
        if ch in CONFUSION_MAP:
            for alt in CONFUSION_MAP[ch]:
                trial = chars.copy()
                trial[i]=alt
                cands.add("".join(trial))
    return [c for c in cands if c in VOCAB]

def correct_spelling_ar(text: str) -> str:
    """ØªØµØ­ÙŠØ­: (1) Ù…Ù† Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„ØµØ±ÙŠØ­ØŒ (2) Ù…Ø±Ø´Ø­Ùˆ Ø§Ù„Ù„Ø¨Ø³ØŒ (3) Ø£Ù‚Ø±Ø¨ Ù…ÙØ±Ø¯Ø© Ø¨Ø§Ù„ÙÙØ²Ù‘ÙŠ."""
    toks = TOKEN_RE.findall(normalize_ar(text))
    out=[]
    for w in toks:
        lw=w.lower()
        if len(lw)<=2 or lw in VOCAB:
            out.append(lw); continue
        # 1) Ù‚Ø§Ù…ÙˆØ³ ØµØ±ÙŠØ­
        if lw in TYPOS_MAP:
            out.append(TYPOS_MAP[lw]); continue
        # 2) Ù…Ø±Ø´Ø­Ùˆ Ø§Ù„Ù„Ø¨Ø³
        vcands = generate_confusion_candidates(lw)
        if vcands:
            # Ø§Ø®ØªØ± Ø§Ù„Ø£Ù‚Ø±Ø¨ Ø´Ø¨Ù‡Ø§Ù‹
            best = process.extractOne(lw, vcands, scorer=fuzz.WRatio)[0]
            out.append(best); continue
        # 3) Ø£Ù‚Ø±Ø¨ Ù…ÙØ±Ø¯Ø© Ø¹Ø§Ù…Ø©
        cand = process.extractOne(lw, VOCAB, scorer=fuzz.WRatio)
        if cand and cand[1] >= 88:
            out.append(cand[0])
        else:
            out.append(lw)
    return " ".join(out)

# --- ÙÙ‡Ø±Ø³Ø© Ù…Ù† Ø¬Ø¯ÙŠØ¯
def reload_all():
    global QA_CACHE, BM25, BM25_DOCS, TFVEC, X_TFIDF
    QA_CACHE = load_qa()
    BM25, BM25_DOCS, TFVEC, X_TFIDF = build_indexes(QA_CACHE)
    load_synonyms(); load_typos()
    build_vocab()

# --- Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø±ÙƒÙ‘Ø¨
def combined_search(user_q: str, topk: int = 5):
    if not QA_CACHE:
        return None, 0.0

    q_base     = normalize_ar(user_q)
    q_correct  = correct_spelling_ar(q_base)     # â¬… ØªØµØ­ÙŠØ­ Ø¥Ù…Ù„Ø§Ø¦ÙŠ
    q_expanded = expand_query(q_correct)         # â¬… ØªÙˆØ³ÙŠØ¹ Ø¯Ù„Ø§Ù„ÙŠ

    q_norm = q_expanded
    q_tok  = ar_tokens(q_norm)
    scores = {}

    # BM25
    if BM25:
        bm = BM25.get_scores(q_tok)
        for i, s in enumerate(bm):
            if s > 0:
                scores[i] = scores.get(i, 0.0) + s * 10.0

    # ØºÙ…ÙˆØ¶ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„
    for i, qa in enumerate(QA_CACHE):
        s = fuzz.token_set_ratio(q_norm, normalize_ar(qa["q"]))  # 0..100
        if s > 0:
            scores[i] = scores.get(i, 0.0) + float(s) * 1.0

    # TF-IDF Ø­Ø±ÙÙŠ (n-grams)
    if TFVEC is not None:
        q_vec = TFVEC.transform([q_norm])
        cos = X_TFIDF @ q_vec.T
        cos = np.asarray(cos.todense()).ravel()
        for i, s in enumerate(cos):
            if s > 0:
                scores[i] = scores.get(i, 0.0) + float(s) * 120.0

    if not scores:
        return None, 0.0

    best_idx, best_score = sorted(scores.items(), key=lambda x: x[1], reverse=True)[0]
    return QA_CACHE[best_idx], best_score

# --- Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
async def local_qa_answer(question: str) -> str:
    question = (question or "").strip()
    if not question:
        return "Ù„Ù… Ø£Ø³ØªÙ„Ù… Ø³Ø¤Ø§Ù„Ù‹Ø§."
    doc, score = combined_search(question, topk=5)
    if doc and score >= 140:
        return doc["a"]
    if doc and score >= 90:
        return f"{doc['a']}\n\n(â„¹ï¸ Ø£Ù‚Ø±Ø¨ Ø³Ø¤Ø§Ù„ Ù…Ø·Ø§Ø¨Ù‚ Ù„Ø¯ÙŠ ÙƒØ§Ù†: Â«{doc['q']}Â»)"
    if doc:
        return (
            f"Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¤ÙƒØ¯Ø©. Ø£Ù‚Ø±Ø¨ Ø³Ø¤Ø§Ù„ Ø¹Ù†Ø¯ÙŠ:\n"
            f"Â«{doc['q']}Â».\n"
            f"Ø§Ù„Ø¬ÙˆØ§Ø¨ Ø§Ù„Ù…Ø®Ø²Ù†: {doc['a']}\n\n"
            f"ÙŠÙ…ÙƒÙ†Ùƒ Ø­ÙØ¸ ØµÙŠØ§ØºØªÙƒ Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ø¹Ø¨Ø± Ø²Ø± Ø§Ù„Ø­ÙØ¸ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙÙ‡Ù… Ù„Ø§Ø­Ù‚Ù‹Ø§."
        )
    return "Ù„Ø§ Ø£Ù…Ù„Ùƒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø¹Ø¯. Ø£Ø¶Ù Ø³/Ø¬ Ù…Ø´Ø§Ø¨Ù‡ Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø£Ùˆ ØºÙŠÙ‘Ø± ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„."

# ========= Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø© =========
def clamp(s: str, n: int) -> str:
    s = (s or "").strip()
    return s if len(s) <= n else s[:n]

# ========= ØµÙØ­Ø§Øª ÙˆØªØ¬Ø±Ø¨Ø© =========
@app.get("/", response_class=HTMLResponse)
def home():
    return f"""
    <div style="max-width:780px;margin:24px auto;font-family:system-ui">
      <h1>ğŸ¤– Bassam Brain â€” Ø§Ù„Ù…Ø­Ù„ÙŠ (Ù…Ø¹ ØªØµØ­ÙŠØ­ Ø¥Ù…Ù„Ø§Ø¦ÙŠ + ÙÙ‡Ù… Ø°ÙƒÙŠ)</h1>
      <form method="post" action="/ask">
        <textarea name="q" rows="5" style="width:100%" placeholder="Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§..."></textarea>
        <div style="margin-top:8px"><button>Ø¥Ø±Ø³Ø§Ù„</button></div>
      </form>

      <details style="margin-top:18px">
        <summary>Ø¥Ø¯Ø§Ø±Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© ÙˆØ§Ù„Ù‚ÙˆØ§Ù…ÙŠØ³</summary>

        <form method="post" action="/save_to_knowledge" style="margin-top:10px">
          <input type="text" name="q" placeholder="Ø³Ø¤Ø§Ù„" style="width:100%;padding:6px"><br>
          <textarea name="a" rows="3" placeholder="Ø¬ÙˆØ§Ø¨" style="width:100%;margin-top:6px"></textarea><br>
          <button>âœ… Ø­ÙØ¸ Ù‡Ø°Ø§ Ø§Ù„Ø¬ÙˆØ§Ø¨ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©</button>
        </form>

        <form method="post" action="/dict/add_typo" style="margin-top:12px">
          <input type="text" name="wrong" placeholder="Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø®Ø§Ø·Ø¦Ø©" style="width:48%;padding:6px">
          <input type="text" name="right" placeholder="Ø§Ù„ØªØµØ­ÙŠØ­ Ø§Ù„ØµØ­ÙŠØ­" style="width:48%;padding:6px;margin-inline-start:4px">
          <button>âœï¸ Ø¥Ø¶Ø§ÙØ© ØªØµØ­ÙŠØ­ Ø¥Ù…Ù„Ø§Ø¦ÙŠ</button>
        </form>
        <p style="margin:6px 0">ØªØ­Ù…ÙŠÙ„: <a href="/export/typos">typos.tsv</a> â€¢ <a href="/export/synonyms">synonyms.txt</a></p>

        <form method="post" action="/reload_all" style="margin-top:10px">
          <button>ğŸ”„ Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙ‡Ø§Ø±Ø³ ÙˆØ§Ù„Ù‚ÙˆØ§Ù…ÙŠØ³</button>
        </form>
      </details>
    </div>
    """

@app.post("/ask", response_class=HTMLResponse)
async def ask(request: Request):
    form = await request.form()
    q     = clamp(form.get("q",""), 1000)
    ans   = await local_qa_answer(q)
    rec = {"ts": int(time.time()), "instruction": q, "input": "", "output": ans}
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(rec, ensure_ascii=False) + "\n")
    return f"""
<div style='max-width:780px;margin:24px auto;font-family:system-ui'>
  <p><b>ğŸ§  Ø³Ø¤Ø§Ù„Ùƒ:</b> {q}</p>
  <p><b>ğŸ’¬ Ø§Ù„Ø¬ÙˆØ§Ø¨:</b> {ans}</p>

  <form method='post' action='/save'>
    <input type='hidden' name='q' value='{q}'>
    <input type='hidden' name='a' value='{ans}'>
    <button style='background:#0d6efd;color:white;padding:8px 16px;border:none;border-radius:6px;cursor:pointer'>ğŸ‘ Ø­ÙØ¸ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨</button>
  </form>

  <form method='post' action='/save_to_knowledge' style='margin-top:8px'>
    <input type='hidden' name='q' value='{q}'>
    <input type='hidden' name='a' value='{ans}'>
    <button style='background:#28a745;color:white;padding:8px 16px;border:none;border-radius:6px;cursor:pointer'>âœ… Ø­ÙØ¸ Ù‡Ø°Ø§ Ø§Ù„Ø¬ÙˆØ§Ø¨ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©</button>
  </form>

  <p style='margin-top:16px'><a href='/'>â—€ Ø±Ø¬ÙˆØ¹</a></p>
</div>
"""

@app.post("/save", response_class=HTMLResponse)
async def save(request: Request):
    form = await request.form()
    rec = {"ts": int(time.time()), "instruction": form.get("q",""), "input": "", "output": form.get("a","")}
    with open(FEED_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(rec, ensure_ascii=False)+"\n")
    return "<p>âœ… ØªÙ…Øª Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø«Ø§Ù„ Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨.</p><p><a href='/'>â—€ Ø±Ø¬ÙˆØ¹</a></p>"

@app.post("/save_to_knowledge", response_class=HTMLResponse)
async def save_to_knowledge(request: Request):
    form = await request.form()
    q = (form.get("q","") or "").strip()
    a = (form.get("a","") or "").strip()
    if not q or not a:
        return "<p>âš ï¸ ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø³Ø¤Ø§Ù„ ÙˆØ¬ÙˆØ§Ø¨.</p><p><a href='/'>â—€ Ø±Ø¬ÙˆØ¹</a></p>"
    entry = f"\nØ³Ø¤Ø§Ù„: {q}\nØ¬ÙˆØ§Ø¨: {a}\n---\n"
    with open(KB_FILE, "a", encoding="utf-8") as f:
        f.write(entry)
    return "<p>âœ… ØªÙ… Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©. Ù„Ø§ ØªÙ†Ø³Ù Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„.</p><p><a href='/'>â—€ Ø±Ø¬ÙˆØ¹</a></p>"

@app.post("/dict/add_typo", response_class=HTMLResponse)
async def add_typo(request: Request):
    form = await request.form()
    wrong = normalize_ar((form.get("wrong","") or "").strip())
    right = normalize_ar((form.get("right","") or "").strip())
    if not wrong or not right:
        return "<p>âš ï¸ Ø£Ø¯Ø®Ù„ Ù‚ÙŠÙ…Ù‹Ø§ ØµØ­ÙŠØ­Ø©.</p><p><a href='/'>â—€ Ø±Ø¬ÙˆØ¹</a></p>"
    with open(TYPOS_FILE, "a", encoding="utf-8") as f:
        f.write(f"{wrong}\t{right}\n")
    return "<p>âœ… Ø£ÙØ¶ÙŠÙ Ø§Ù„ØªØµØ­ÙŠØ­ Ø¥Ù„Ù‰ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³. Ø§Ø¶ØºØ· Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù„ØªÙØ¹ÙŠÙ„Ù‡.</p><p><a href='/'>â—€ Ø±Ø¬ÙˆØ¹</a></p>"

@app.post("/reload_all")
def reload_all_endpoint():
    reload_all()
    return {"ok": True, "qa_count": len(QA_CACHE), "vocab": len(VOCAB)}

# ===== API =====
@app.get("/ready")
def ready(): return {"ok": True}

@app.post("/answer")
async def answer(req: Request):
    body = await req.json()
    q = (body.get("question") or "").strip()
    if not q:
        raise HTTPException(status_code=400, detail="Ø¶Ø¹ Ø­Ù‚Ù„ 'question'")
    text = await local_qa_answer(q)
    rec  = {"ts": int(time.time()), "instruction": q, "input": "", "output": text}
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(rec, ensure_ascii=False)+"\n")
    return {"ok": True, "answer": text}

# ØªÙ†Ø²ÙŠÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„Ù‚Ø§Ù…ÙˆØ³
@app.get("/export/typos")
def export_typos():
    return FileResponse(path=str(TYPOS_FILE), media_type="text/plain", filename="typos.tsv")

@app.get("/export/synonyms")
def export_synonyms():
    return FileResponse(path=str(SYN_FILE), media_type="text/plain", filename="synonyms.txt")
